{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDA Assignment #2: MapReduce\n",
    "## Calculating the Term Frequency (TF), and Inverse Document Frequency (IDF).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group members: \n",
    "Aaqib Ahmed Nazir (i22-1920),  \n",
    "Arhum Khan (i22-1967), \n",
    "Ammar Khasif (i22-1968)\n",
    "\n",
    "##### Section: DS-D   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries Useds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function To Genrate Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_vocabulary(file_path, chunk_size):\n",
    "#     # divide the file into chunks\n",
    "#     chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "#     vocabulary = set()\n",
    "    \n",
    "#     for chunk in chunks:\n",
    "#         for index, row in chunk.iterrows():\n",
    "#             words = row['SECTION_TEXT'].split()\n",
    "#             # adding the words to the vocabulary\n",
    "#             vocabulary.update(words) \n",
    "#     return vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function To Save Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_vocabulary(vocabulary, file_path):\n",
    "#     # save the vocabulary to a file\n",
    "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#         for word in vocabulary:\n",
    "#             f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a vocabulary from the given dataset and saving it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/home/arhum/Downloads/preprocessed_dataset.csv\"\n",
    "# chunk_size = 10000\n",
    "# vocabulary = make_vocabulary(file_path, chunk_size)\n",
    "# save_vocabulary(vocabulary, \"vocabulary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing purposes, we will use the first 1000 lines of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARTICLE_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>SECTION_TITLE</th>\n",
       "      <th>SECTION_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>anarchism political philosophy advocates selfg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>Etymology and terminology</td>\n",
       "      <td>term anarchism compound word composed word ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>History</td>\n",
       "      <td>zzorigins woodcut diggers document william eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>Anarchist schools of thought</td>\n",
       "      <td>portrait philosopher pierrejoseph proudhon 180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>Internal issues and debates</td>\n",
       "      <td>consistent anarchist values controversial subj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ARTICLE_ID      TITLE                 SECTION_TITLE  \\\n",
       "0           0  Anarchism                  Introduction   \n",
       "1           0  Anarchism     Etymology and terminology   \n",
       "2           0  Anarchism                       History   \n",
       "3           0  Anarchism  Anarchist schools of thought   \n",
       "4           0  Anarchism   Internal issues and debates   \n",
       "\n",
       "                                        SECTION_TEXT  \n",
       "0  anarchism political philosophy advocates selfg...  \n",
       "1  term anarchism compound word composed word ana...  \n",
       "2  zzorigins woodcut diggers document william eve...  \n",
       "3  portrait philosopher pierrejoseph proudhon 180...  \n",
       "4  consistent anarchist values controversial subj...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking a chunk of 10000 rows from the dataset\n",
    "chunk_size = 10000\n",
    "file_path = \"C:\\\\Users\\\\Arhum Khan\\\\Desktop\\\\preprocessed_dataset.csv\"\n",
    "chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "chunks = next(chunks)\n",
    "\n",
    "display(chunks.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function make a vocabulary from the given dataset and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177860"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a vocabulary of the chunk\n",
    "vocabulary = set()\n",
    "for index, row in chunks.iterrows():\n",
    "    words = row['SECTION_TEXT'].split()\n",
    "    for word in words:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.add(word)\n",
    "            \n",
    "            \n",
    "# saving the vocabulary to a file\n",
    "with open(\"vocabulary.txt\", 'w', encoding='utf-8') as f:\n",
    "    for word in vocabulary:\n",
    "        f.write(word + '\\n')\n",
    "        \n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning all the words in the vocabulary a unique index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of word to id and id to word\n",
    "word_to_id = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "\n",
    "# saving to a csv file\n",
    "word_to_id_df = pd.DataFrame(list(word_to_id.items()), columns=['word', 'id'])\n",
    "word_to_id_df.to_csv('Word_IDs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency (TF) is a measure of how often a word appears in a document. It is calculated as the number of times a word appears in a document divided by the total number of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_word_counts = {}\n",
    "\n",
    "for index, row in chunks.iterrows():\n",
    "    # get the word counts for each article\n",
    "    article_id = row['ARTICLE_ID']\n",
    "    words = row['SECTION_TEXT'].split()\n",
    "    word_count = {}\n",
    "    \n",
    "    for word in words:\n",
    "        word_id = word_to_id[word]\n",
    "        if word_id in word_count:\n",
    "            word_count[word_id] += 1\n",
    "        else:\n",
    "            word_count[word_id] = 1\n",
    "    # save the word counts for each article\n",
    "    article_word_counts[article_id] = [(word_id, count) for word_id, count in word_count.items()]\n",
    "    \n",
    "# saving to a csv file\n",
    "article_word_counts_df = pd.DataFrame([(article_id, word_id, count) for article_id, word_counts in article_word_counts.items() for word_id, count in word_counts], columns=['article_id', 'word_id', 'frequency'])\n",
    "article_word_counts_df.to_csv('TF.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given word, the Inverse Document Frequency (IDF) is the logarithm of the total number of documents divided by the number of documents containing the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of articles in which each word appears\n",
    "word_frequency = {}\n",
    "for article_id, word_counts in article_word_counts.items():\n",
    "    for word_id, count in word_counts:\n",
    "        if word_id in word_frequency:\n",
    "            word_frequency[word_id] += 1\n",
    "        else:\n",
    "            word_frequency[word_id] = 1\n",
    "            \n",
    "# save the word_frequency dictionary to a csv file with word_id and frequency as columns\n",
    "word_frequency_df = pd.DataFrame(list(word_frequency.items()), columns=['word_id', 'frequency'])\n",
    "word_frequency_df.to_csv('IDF.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the TF-IDF Weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the files\n",
    "tf_df = pd.read_csv(\"TF.csv\")\n",
    "idf_df = pd.read_csv(\"IDF.csv\")\n",
    "\n",
    "# Calculate the TF-IDF values\n",
    "# the TF-IDF score for each word in each article is the term frequency of the word in the whole dataset divided by the IDF of the word\n",
    "# we need to add the term frequency of the word if it is appearing in 2 different articles then divide by the IDF of the word\n",
    "\n",
    "tf_idf = []\n",
    "for index, row in tf_df.iterrows():\n",
    "    article_id = row['article_id']\n",
    "    word_id = row['word_id']\n",
    "    frequency = row['frequency']\n",
    "    tf = frequency\n",
    "    idf = idf_df[idf_df['word_id'] == word_id]['frequency'].values[0]\n",
    "    tf_idf.append((article_id, word_id, tf/idf))\n",
    "    \n",
    "# save the tf_idf values to a csv file\n",
    "tf_idf_df = pd.DataFrame(tf_idf, columns=['article_id', 'word_id', 'tf_idf'])\n",
    "tf_idf_df.to_csv('TF_IDF.csv', index=False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
