{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDA Assignment #2: MapReduce\n",
    "## Calculating the Term Frequency (TF), and Inverse Document Frequency (IDF).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group members: \n",
    "Aaqib Ahmed Nazir (i22-1920),  \n",
    "Arhum Khan (i22-1967), \n",
    "Ammar Khasif (i22-1968)\n",
    "\n",
    "##### Section: DS-D   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARTICLE_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>SECTION_TITLE</th>\n",
       "      <th>SECTION_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>anarchism anarchism is the only thing that mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>Origins</td>\n",
       "      <td>the only thing matters except for anarchism is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>History</td>\n",
       "      <td>historical anarchism does not matter in this w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "      <td>Origins</td>\n",
       "      <td>world is the only thing that matters to people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "      <td>History</td>\n",
       "      <td>world history is not the same as historical an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ARTICLE_ID      TITLE SECTION_TITLE  \\\n",
       "0           0  Anarchism  Introduction   \n",
       "1           0  Anarchism       Origins   \n",
       "2           0  Anarchism       History   \n",
       "3           1      World       Origins   \n",
       "4           1      World       History   \n",
       "\n",
       "                                        SECTION_TEXT  \n",
       "0  anarchism anarchism is the only thing that mat...  \n",
       "1  the only thing matters except for anarchism is...  \n",
       "2  historical anarchism does not matter in this w...  \n",
       "3     world is the only thing that matters to people  \n",
       "4  world history is not the same as historical an...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking a chunk of 10000 rows from the dataset\n",
    "file_path = \"./Dataset/simplified_data.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a vocabulary of all the words in the documents and assigning a unique ID to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a vocabulary\n",
    "vocabulary = set()\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    words = row['SECTION_TEXT'].split()\n",
    "\n",
    "    for word in words:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.add(word)\n",
    "\n",
    "# saving the vocabulary to a file\n",
    "with open(\"./Dataset/vocabulary.txt\", 'w', encoding='utf-8') as f:\n",
    "    for word in vocabulary:\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "# assigning an ID to each word\n",
    "word_to_id = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# saving the word_to_id mapping to a file\n",
    "with open(\"./Dataset/vocab_id.csv\", 'w', encoding='utf-8') as f:\n",
    "    for word, idx in word_to_id.items():\n",
    "        f.write(f\"{word},{idx}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency (TF) is a measure of how often a word appears in a document. It is calculated as the number of times a word appears in a document divided by the total number of words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_word_counts = {}\n",
    "prev_article_id = None\n",
    "word_count = {}\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    # get the word counts for each article\n",
    "    article_id = row['ARTICLE_ID']\n",
    "    words = row['SECTION_TEXT'].split()\n",
    "\n",
    "    if article_id != prev_article_id:\n",
    "        word_count = {}\n",
    "        prev_article_id = article_id\n",
    "    \n",
    "    for word in words:\n",
    "        word_id = word_to_id[word]\n",
    "        if word_id in word_count:\n",
    "            word_count[word_id] += 1\n",
    "        else:\n",
    "            word_count[word_id] = 1\n",
    "    \n",
    "    # save the word counts for each article\n",
    "    article_word_counts[article_id] = [(word_id, count) for word_id, count in word_count.items()]\n",
    "\n",
    "    \n",
    "# saving term frequencies to a csv file\n",
    "article_word_counts_df = pd.DataFrame([(article_id, word_id, count) for article_id, word_counts in article_word_counts.items() for word_id, count in word_counts], columns=['article_id', 'word_id', 'frequency'])\n",
    "article_word_counts_df.to_csv('./Dataset/term_frequencies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given word, the Inverse Document Frequency (IDF) is the logarithm of the total number of documents divided by the number of documents containing the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = {}\n",
    "\n",
    "# count the number of articles in which each word appears\n",
    "for article_id, word_counts in article_word_counts.items():\n",
    "    for word_id, count in word_counts:\n",
    "        if word_id in word_frequency:\n",
    "            word_frequency[word_id] += 1\n",
    "        else:\n",
    "            word_frequency[word_id] = 1\n",
    "            \n",
    "# save the word_frequency dictionary to a csv file\n",
    "word_frequency_df = pd.DataFrame(list(word_frequency.items()), columns=['word_id', 'frequency'])\n",
    "word_frequency_df.to_csv('./Dataset/inverse_doc_frequencies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the TF-IDF Weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tf and idf files\n",
    "tf_df = pd.read_csv(\"./Dataset/term_frequencies.csv\")\n",
    "idf_df = pd.read_csv(\"./Dataset/inverse_doc_frequencies.csv\")\n",
    "\n",
    "weights = []\n",
    "for index, row in tf_df.iterrows():\n",
    "    # get tf and idf values for each word\n",
    "    article_id = row['article_id']\n",
    "    word_id = row['word_id']\n",
    "    tf = row['frequency']\n",
    "    idf = idf_df[idf_df['word_id'] == word_id]['frequency'].values[0]\n",
    "\n",
    "    weights.append((article_id, word_id, tf/idf))\n",
    "    \n",
    "# save the tf_idf values to a csv file\n",
    "tf_idf_df = pd.DataFrame(weights, columns=['article_id', 'word_id', 'tf_idf'])\n",
    "tf_idf_df.to_csv('./Dataset/weights.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the sparse vector representation for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID: 0.0, Vector: {8.0: 2.0, 10.0: 1.0, 21.0: 1.5, 12.0: 1.5, 9.0: 1.5, 22.0: 1.0, 4.0: 1.5, 2.0: 1.0, 0.0: 1.0, 11.0: 2.0, 18.0: 1.0, 6.0: 1.0, 13.0: 0.5, 1.0: 1.0, 17.0: 0.5, 7.0: 1.0, 14.0: 1.0, 23.0: 1.0, 20.0: 0.5}\n",
      "Article ID: 1.0, Vector: {20.0: 1.0, 10.0: 1.0, 21.0: 1.0, 12.0: 0.5, 9.0: 0.5, 22.0: 0.5, 4.0: 0.5, 5.0: 1.0, 16.0: 1.0, 15.0: 1.0, 17.0: 0.5, 19.0: 1.0, 3.0: 1.0, 13.0: 0.5, 8.0: 0.5}\n"
     ]
    }
   ],
   "source": [
    "# initialize the sparse vector dictionary\n",
    "sparse_vectors = {}\n",
    "\n",
    "# iterate over the tf_idf dataframe\n",
    "for index, row in tf_idf_df.iterrows():\n",
    "    article_id = row['article_id']\n",
    "    word_id = row['word_id']\n",
    "    tf_idf = row['tf_idf']\n",
    "\n",
    "    # if the article_id is not in the dictionary, add it\n",
    "    if article_id not in sparse_vectors:\n",
    "        sparse_vectors[article_id] = {}\n",
    "\n",
    "    # add the word_id and its tf_idf weight to the inner dictionary\n",
    "    sparse_vectors[article_id][word_id] = tf_idf\n",
    "\n",
    "# print the sparse vector representation for each document\n",
    "for article_id, vector in sparse_vectors.items():\n",
    "    print(f\"Article ID: {article_id}, Vector: {vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting query to a vector using the TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(21, 0.5)]\n"
     ]
    }
   ],
   "source": [
    "# query\n",
    "query = \"Travels around the wolrd\"\n",
    "\n",
    "# clean the query by removing punctuation, converting to lowercase and splitting into words\n",
    "query = query.replace(',', '').replace('.', '').lower().split()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the query words\n",
    "lemmatized_query = [lemmatizer.lemmatize(word) for word in query]\n",
    "\n",
    "# get the word ids for the lemmatized query words\n",
    "lemmatized_query_word_ids = []\n",
    "for word in lemmatized_query:\n",
    "    if word in word_to_id:\n",
    "        lemmatized_query_word_ids.append(word_to_id[word])\n",
    "\n",
    "# get the tf_idf values for the lemmatized query words\n",
    "lemmatized_query_weights = []\n",
    "for word_id in lemmatized_query_word_ids:\n",
    "    if word_id in word_frequency:\n",
    "        idf = word_frequency[word_id]\n",
    "        lemmatized_query_weights.append((word_id, 1/idf))\n",
    "\n",
    "print(lemmatized_query_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the similarity between the query vector and the document vectors using scalar product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0: 0.75\n",
      "Article 1: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Calculating the similarity between the query vector and the document vectors using scalar product\n",
    "similarities = {}\n",
    "for article_id, vector in sparse_vectors.items():\n",
    "    similarity = 0\n",
    "    for word_id, weight in lemmatized_query_weights:\n",
    "        if word_id in vector:\n",
    "            similarity += weight * vector[word_id]\n",
    "\n",
    "    similarities[article_id] = similarity\n",
    "\n",
    "# sort the similarities\n",
    "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print the top 5 most similar articles\n",
    "for article_id, similarity in sorted_similarities[:5]:\n",
    "    print(f\"Article {int(article_id)}: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
